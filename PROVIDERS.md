# ğŸ¤– AI Provider Guide

**Browser AI** supports multiple AI providers. Choose based on your needs:

---

## ğŸ¯ Quick Comparison

| Provider | Privacy | Cost | Speed | Quality | Setup Time |
|----------|---------|------|-------|---------|------------|
| **OpenAI** | âš ï¸ Cloud | ğŸ’° ~$0.03/req | âš¡ Fast | â­â­â­â­â­ | 5 min |
| **Ollama** | ğŸ”’ 100% Local | ğŸ†“ FREE | ğŸ¢ Slow | â­â­â­â­ | 15 min |
| **Local LLM** | ğŸ”’ 100% Local | ğŸ†“ FREE | ğŸ¢ Very Slow | â­â­â­ | 30+ min |

---

## ğŸ¨ Provider Options

### Option 1: OpenAI GPT-4 Vision (Cloud) â˜ï¸

**Best for:**
- Highest quality results
- Fast response times
- Production use
- When privacy is less critical

**Pros:**
- âœ… Best AI quality available
- âœ… Very fast (2-5 seconds)
- âœ… Reliable and stable
- âœ… Well-documented API
- âœ… Easy to set up

**Cons:**
- âŒ Costs money (~$0.02-$0.05 per request)
- âŒ Data sent to OpenAI servers
- âŒ Requires internet connection
- âŒ API key required

**Setup:**

1. **Get API Key**
   - Go to https://platform.openai.com/api-keys
   - Create account (requires payment method)
   - Create new API key
   - Copy the key (starts with `sk-`)

2. **Configure Backend**
   ```bash
   cd backend
   cp env-template.txt .env
   # Edit .env and add:
   OPENAI_API_KEY=sk-your-actual-key-here
   ```

3. **Start Backend**
   ```bash
   python server.py
   ```

4. **Select in Browser**
   - Open AI Panel settings
   - Select "OpenAI GPT-4 Vision"
   - Done! ğŸ‰

**Cost Estimate:**
- $0.01275 per image
- $0.01 per 1K input tokens
- $0.03 per 1K output tokens
- **Total: ~$0.02-$0.05 per automation request**

---

### Option 2: Ollama (Local) ğŸ”’

**Best for:**
- Privacy-conscious users
- Free usage
- Offline capability
- Development/testing

**Pros:**
- âœ… 100% private (never leaves your machine)
- âœ… FREE forever
- âœ… Works offline
- âœ… No API key needed
- âœ… Pretty good quality
- âœ… Easy to set up

**Cons:**
- âŒ Slower (5-30 seconds per request)
- âŒ Requires GPU for good speed
- âŒ Lower quality than OpenAI
- âŒ Larger disk space needed (~4-7GB per model)

**Setup:**

1. **Install Ollama**
   - Windows: Download from https://ollama.ai
   - Run installer
   - Ollama will start automatically

2. **Download Vision Model**
   ```bash
   # LLaVA (recommended, 4GB)
   ollama pull llava
   
   # OR BakLLaVA (alternative, 7GB)
   ollama pull bakllava
   ```

3. **Verify Installation**
   ```bash
   ollama list
   # Should show: llava
   ```

4. **Start Backend**
   ```bash
   cd backend
   python server.py
   ```

5. **Select in Browser**
   - Open AI Panel settings
   - Select "Ollama (Local & Private)"
   - Done! ğŸ‰

**Performance Tips:**

- **For CPU-only (slow):**
  - First request: 30-60 seconds
  - Subsequent: 20-30 seconds
  - Use smaller models

- **With GPU (fast):**
  - First request: 5-10 seconds
  - Subsequent: 3-5 seconds
  - Much better quality possible

**Troubleshooting:**

```bash
# Check if Ollama is running:
curl http://localhost:11434/api/tags

# If not running, start it:
ollama serve

# Check models:
ollama list

# Test a model:
ollama run llava "Describe this image" < test.png
```

---

### Option 3: Local LLM (Native) ğŸ”§

**Best for:**
- Maximum privacy
- Custom models
- Advanced users
- Research/development

**Pros:**
- âœ… 100% private
- âœ… FREE
- âœ… Full control
- âœ… Custom models
- âœ… Works offline

**Cons:**
- âŒ Most complex setup
- âŒ Slowest option
- âŒ Requires technical knowledge
- âŒ Quality varies by model
- âŒ Resource intensive

**Setup:**

1. **Build Automation Service**
   ```bash
   cd automation_service
   ./build.bat
   ```

2. **Configure for Local LLM**
   - Edit `automation_service/src/main.cpp`
   - Add local inference backend (llama.cpp, etc.)
   - Rebuild

3. **Register Native Messaging**
   ```bash
   cd automation_service
   ./register-manifest.bat
   ```

4. **Select in Browser**
   - Open AI Panel settings
   - Select "Local LLM (Privacy)"
   - Configure model path

**This option requires C++ development knowledge!**

---

## ğŸšï¸ Choosing Your Provider

### For Most Users: OpenAI
If you're okay with cloud AI and can afford $5-10/month for occasional use.

### For Privacy Fans: Ollama
Best balance of privacy, ease of use, and quality.

### For Developers: All Three
Set up multiple providers and switch based on the task!

---

## ğŸ”„ Switching Providers

**You can switch providers at any time:**

1. Click gear icon in AI Panel
2. Select different provider from dropdown
3. Provider is saved automatically
4. All actions will use new provider

**Example workflow:**

- **Development:** Use Ollama (free)
- **Testing:** Use OpenAI (quality validation)
- **Production:** Use whatever fits your needs

---

## ğŸ”’ Privacy Comparison

### What Data is Shared?

| Provider | Screenshot | UI Tree | Your Prompts | Stored? | Duration |
|----------|------------|---------|--------------|---------|----------|
| **OpenAI** | âœ“ Sent | âœ“ Sent | âœ“ Sent | Yes | 30 days |
| **Ollama** | âœ— Local | âœ— Local | âœ— Local | No | N/A |
| **Local LLM** | âœ— Local | âœ— Local | âœ— Local | No | N/A |

### Security Features

**All Providers:**
- âœ… HTTPS/secure connections
- âœ… No data in browser localStorage
- âœ… API keys stored server-side only
- âœ… Audit logging available

**Local Providers (Ollama/Local LLM):**
- âœ… Never touches the internet
- âœ… No third-party services
- âœ… You control all data
- âœ… Can be used offline

---

## ğŸ’° Cost Analysis

### OpenAI Monthly Costs

**Light use** (10 requests/day):
- ~300 requests/month
- ~$6-15/month

**Moderate use** (50 requests/day):
- ~1,500 requests/month
- ~$30-75/month

**Heavy use** (200 requests/day):
- ~6,000 requests/month
- ~$120-300/month

### Ollama Costs

**Setup:**
- $0 (free download)

**Monthly:**
- $0 (no recurring costs)

**Hardware:**
- Works on any PC (slow without GPU)
- Much faster with GPU (but not required)

**Electricity:**
- ~0.5-2 kWh per hour of use
- ~$0.05-0.20 per hour (depending on rates)

---

## ğŸ§ª Testing Providers

### Test OpenAI

```bash
cd backend
python test_backend.py
```

Expected output:
```
âœ… Health check passed
âœ… Providers list retrieved
âœ… OpenAI: Received 3 actions
```

### Test Ollama

```bash
cd backend
python test_backend.py
```

Expected output:
```
âœ… Health check passed
âœ… Providers list retrieved
â­ï¸ OpenAI not configured (skipping)
âœ… Ollama: Received 2 actions
```

---

## ğŸ“ Provider Architecture

### How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser   â”‚ (AI Panel UI)
â”‚   (Layer 1) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend  â”‚  â”‚    Native    â”‚
â”‚   Proxy    â”‚  â”‚  Messaging   â”‚
â”‚  (Layer 3) â”‚  â”‚  (Layer 2)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚            â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”     â”‚
   â”‚         â”‚     â”‚
â”Œâ”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â–¼â”€â”€â”  â–¼
â”‚OpenAIâ”‚  â”‚Oll-â”‚  Local
â”‚ API  â”‚  â”‚ama â”‚  LLM
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜
```

### Why Backend Proxy?

**Security:**
- âœ… API keys never in browser
- âœ… Keys can't be extracted by users
- âœ… Centralized key management

**Flexibility:**
- âœ… Easy to add new providers
- âœ… Can switch without browser changes
- âœ… Rate limiting, caching, logging

**Privacy:**
- âœ… Filter sensitive data before sending
- âœ… Audit all requests
- âœ… Can add encryption

---

## ğŸ”® Future Providers

We're planning to add:

- [ ] **Anthropic Claude** (cloud, OpenAI alternative)
- [ ] **Google Gemini** (cloud, Google's AI)
- [ ] **LM Studio** (local, GUI for models)
- [ ] **Azure OpenAI** (enterprise, Microsoft)
- [ ] **Hugging Face** (cloud/local, open models)
- [ ] **Custom API** (bring your own provider!)

**Want to add a provider?** PRs welcome!

---

## ğŸ¤ Recommendations

### For Getting Started

1. **Start with OpenAI** (easiest, best quality)
   - Get something working quickly
   - Validate your use case
   - See if it's worth investing more time

2. **Then Try Ollama** (free, private)
   - See if quality is good enough
   - Test performance on your hardware
   - Decide if you need cloud AI

3. **Use Both** (best of both worlds)
   - OpenAI for important tasks
   - Ollama for privacy-sensitive tasks
   - Switch freely in the UI

### For Production

- **Need reliability:** OpenAI
- **Need privacy:** Ollama
- **Need both:** Run Ollama, failover to OpenAI
- **Enterprise:** Consider Azure OpenAI (not yet supported)

---

## ğŸ“š Additional Resources

### OpenAI
- API Docs: https://platform.openai.com/docs/guides/vision
- Pricing: https://openai.com/pricing
- Rate Limits: https://platform.openai.com/docs/guides/rate-limits

### Ollama
- Website: https://ollama.ai
- Model Library: https://ollama.ai/library
- GitHub: https://github.com/ollama/ollama
- Discord: https://discord.gg/ollama

### Local LLMs
- llama.cpp: https://github.com/ggerganov/llama.cpp
- LLaVA: https://llava-vl.github.io/
- CogVLM: https://github.com/THUDM/CogVLM
- Qwen-VL: https://github.com/QwenLM/Qwen-VL

---

## â“ FAQ

### Q: Which provider should I use?

**A:** Start with OpenAI if you can afford it (~$10/month light use). Switch to Ollama if you want free and private.

### Q: Can I use multiple providers?

**A:** Yes! Set up as many as you want and switch in the UI.

### Q: Is my data safe with OpenAI?

**A:** OpenAI stores data for 30 days for abuse monitoring. After that, it's deleted. Read their privacy policy: https://openai.com/policies/privacy-policy

### Q: How much faster is Ollama with a GPU?

**A:** 5-10x faster. CPU: 20-30 sec/request, GPU: 2-5 sec/request.

### Q: Can I use Ollama on a laptop?

**A:** Yes, but it will be slow without a dedicated GPU. Still faster than waiting for cloud API if you have no internet!

### Q: What's the best Ollama model?

**A:** `llava` (4GB) for most users. `bakllava` (7GB) for better quality if you have space.

### Q: Can I add my own AI provider?

**A:** Yes! See `backend/README.md` for instructions on adding custom providers.

### Q: Does this work offline?

**A:** OpenAI requires internet. Ollama and Local LLM work 100% offline.

---

**Questions?** Open an issue or check the main README!

